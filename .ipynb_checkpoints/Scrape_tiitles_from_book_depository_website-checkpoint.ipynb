{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f58fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106e4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bookdepository.com/bestsellers\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba99861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_titles = soup.find_all(\"h3\",class_ = \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970ef3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It Ends With Us: The most heartbreaking novel you'll ever read\",\n",
       " 'The Midnight Library',\n",
       " \"Heaven Official's Blessing\",\n",
       " \"You've Reached Sam\",\n",
       " 'Grandmaster of Demonic Cultivation',\n",
       " 'The Spanish Love Deception',\n",
       " 'Ugly Love',\n",
       " 'Demon Slayer Complete Box Set',\n",
       " 'Seven Husbands of Evelyn Hugo',\n",
       " 'Flavourbomb',\n",
       " 'The Christmas Pig',\n",
       " \"The Scum Villain's Self-Saving System\",\n",
       " 'The Song of Achilles',\n",
       " 'The Love Hypothesis',\n",
       " 'This Much is True',\n",
       " 'The Thursday Murder Club',\n",
       " 'No Longer Human',\n",
       " 'Animal Farm',\n",
       " 'The Promise',\n",
       " 'The Hating Game',\n",
       " 'Atomic Habits',\n",
       " 'The Storyteller',\n",
       " 'November 9',\n",
       " 'The Real Anthony Fauci',\n",
       " 'Where the Crawdads Sing',\n",
       " 'Diary of a Wimpy Kid: Big Shot (Book 16)',\n",
       " 'The Body Keeps the Score',\n",
       " 'Dune',\n",
       " \"Giraffes Can't Dance\",\n",
       " \"Joy's Playground\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "for i in parent_titles:\n",
    "    titles.append(i.findChild().text.strip())\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d23388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02 Aug 2016',\n",
       " '18 Feb 2021',\n",
       " '14 Dec 2021',\n",
       " '01 Jan 2022',\n",
       " '14 Dec 2021',\n",
       " '28 Oct 2021',\n",
       " '21 Jan 2016',\n",
       " '09 Dec 2021',\n",
       " '14 Oct 2021',\n",
       " '01 Mar 2022',\n",
       " '12 Oct 2021',\n",
       " '14 Dec 2021',\n",
       " '21 Sep 2017',\n",
       " '28 Sep 2021',\n",
       " '10 May 2022',\n",
       " '13 May 2021',\n",
       " '01 Jan 2020',\n",
       " '01 Oct 2008',\n",
       " '17 Jun 2021',\n",
       " '07 Sep 2017',\n",
       " '27 Nov 2018',\n",
       " '05 Oct 2021',\n",
       " '10 Nov 2015',\n",
       " '16 Nov 2021',\n",
       " '20 Dec 2019',\n",
       " '26 Oct 2021',\n",
       " '24 Sep 2015',\n",
       " '15 Jul 2011',\n",
       " '01 May 2014',\n",
       " '26 Oct 2021']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_dates = soup.find_all('p',class_ = \"published\")\n",
    "dates = []\n",
    "for i in published_dates:\n",
    "    dates.append(i.text)\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a2a5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹838.7',\n",
       " '₹761.5',\n",
       " '₹1,331',\n",
       " '₹750.8',\n",
       " '₹1,331',\n",
       " '₹2,026',\n",
       " '₹823.1',\n",
       " '₹13,32',\n",
       " '₹851.2',\n",
       " '₹2,198',\n",
       " '₹1,889',\n",
       " '₹1,331',\n",
       " '₹887.3',\n",
       " '₹1,178',\n",
       " '₹2,079',\n",
       " '₹835.9',\n",
       " '₹994.7',\n",
       " '₹334.1',\n",
       " '₹1,772',\n",
       " '₹706.6',\n",
       " '₹1,424',\n",
       " '₹1,888',\n",
       " '₹1,098',\n",
       " '₹2,427',\n",
       " '₹748.3',\n",
       " '₹1,097',\n",
       " '₹1,123',\n",
       " '₹1,165',\n",
       " '₹722.7',\n",
       " '₹637.6']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prices = soup.find_all('p',class_ = \"price\")\n",
    "prices = []\n",
    "for i in all_prices:\n",
    "    prices.append(i.text.strip()[:6])\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4810e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef84e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sno = range(1,len(titles)+1)\n",
    "df = pd.DataFrame({\"Book Title\":titles,\"Published Date\":dates,\"Prices\":prices}, index = sno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c31529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Best_Sellers_Book.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
